# 🗄️ 搜索增强指南

## 概述

LLMSider 的搜索增强功能通过 Embedding 技术为您的笔记库提供语义搜索能力，让 AI 能够自动查找和使用相关信息。这不是传统的关键词搜索，而是基于内容语义理解的智能检索。

---

## 🎯 什么是语义搜索？

### 传统搜索 vs 语义搜索

**传统（关键词）搜索：**
```
查询: "人工智能应用"
匹配: 包含这些确切单词的笔记
```

**语义搜索：**
```
查询: "人工智能应用"
匹配: 关于以下内容的笔记：
  - 机器学习用例
  - 医疗保健中的 AI
  - 神经网络部署
  - 即使它们不包含确切的单词！
```

### 工作原理

```
1. 您的笔记 → 2. 分割成块 → 3. 转换为向量
   ┌─────────┐      ┌──────────┐        ┌───────────┐
   │ Note.md │ →    │ 块 1     │ →      │ [0.2, ... │
   │         │      │ 块 2     │        │ [0.5, ... │
   └─────────┘      └──────────┘        └───────────┘

4. 存储在本地索引 ← 5. AI 查询 ← 6. 获取相关上下文
   ┌────────────┐      ┌──────────┐       ┌───────────┐
   │ Orama 索引 │ ←    │  查询    │  ←   │ AI 找到   │
   │ 已索引     │      │  向量    │       │ 相似的    │
   └────────────┘      └──────────┘       └───────────┘
```

---

## 🚀 设置指南

### 步骤 1：启用搜索增强

1. **打开设置**
   - 设置 → LLMSider → 搜索增强

2. **开启增强搜索**
   - 切换"开启增强搜索"开关
   - 系统将开始后台 Embedding 化处理

3. **配置 Embedding 模型**
   - 选择 Embedding 提供商：
     - **OpenAI** (text-embedding-3-small) - 推荐，质量高
     - **本地 Transformer 模型** (免费、隐私保护)
     - **Ollama** (本地运行)

### 步骤 2：选择 Embedding 提供商

#### 选项 A：OpenAI（推荐）

**设置：**
1. 创建 OpenAI 连接（如果不存在）
2. 添加 Embedding 模型：
   ```yaml
   模型名称: text-embedding-3-small
   类型: Embedding
   维度: 1536
   ```

**优点：**
- ✅ 高质量语义理解
- ✅ 速度快
- ✅ 稳定可靠
- ✅ 性价比高

**缺点：**
- ❌ 需要 API 密钥
- ❌ 数据发送到 OpenAI
- ❌ 有小额使用成本

#### 选项 B：本地 Transformer 模型（免费）

**设置：**
1. 首次使用时会自动下载模型
2. 模型存储在本地，完全离线运行
3. 推荐模型：
   ```yaml
   Xenova/all-MiniLM-L6-v2
   维度: 384
   ```

**优点：**
- ✅ 完全免费
- ✅ 隐私保护（本地处理）
- ✅ 无需 API 密钥
- ✅ 离线可用

**缺点：**
- ❌ 首次下载模型需要时间
- ❌ 质量略低于 OpenAI
- ❌ 占用本地存储空间

#### 选项 C：Ollama（本地）

**设置：**
1. 安装 Ollama
2. 拉取 Embedding 模型：
   ```bash
   ollama pull mxbai-embed-large
   ```
3. 配置 LLMSider：
   ```yaml
   连接: Ollama
   模型: mxbai-embed-large
   维度: 1024
   ```

**优点：**
- ✅ 免费
- ✅ 私密（本地）
- ✅ 无需 API
- ✅ 硬件好的情况下速度快

**缺点：**
- ❌ 需要本地计算资源
- ❌ 设置较复杂

### 步骤 3：配置分块策略

LLMSider 提供两种分块策略：

#### 1. 语义分块（推荐）
```yaml
策略: 语义分块
描述: 根据文档结构（标题、段落）自动分割
优点: 保持语义连贯性，无需手动配置
适合: 结构化的 Markdown 笔记
```

#### 2. 字符分块
```yaml
策略: 字符分块
分块大小: 1000（默认，范围 100-5000）
分块重叠: 100（默认，必须小于分块大小）
适合: 需要精确控制分块粒度的场景
```

### 步骤 4：初始索引构建

1. 点击"重建索引（完整）"
2. 等待完成（显示进度）
3. 索引准备就绪！

**进度指示器：**
```
📊 正在索引笔记库...
─────────────────────────────── 45%
处理文件 125 / 280
索引块 567 / 1247
```

---

## 🎨 核心功能

### 1. 相似笔记显示

**在设置中启用：**
```yaml
显示相似笔记: ✓
默认隐藏相似笔记: ✓（鼠标悬停显示）
```

**效果：**
- 在笔记底部显示语义相关的其他笔记
- 基于内容相似度自动排序
- 点击快速跳转

```
┌─────────────────────────────┐
│ 📊 相似笔记 (3)             │
├─────────────────────────────┤
│ 🔗 机器学习基础 (92%)       │
│ 🔗 神经网络原理 (87%)       │
│ 🔗 AI 伦理思考 (81%)        │
└─────────────────────────────┘
```

### 2. AI 对话自动增强

**在聊天中：**
```
您: "我之前写过关于项目管理的内容吗？"
     ↓
搜索增强自动查找相关笔记
     ↓
AI 基于找到的笔记内容回答
```

**效果对比：**

未开启本地搜索增强时：
![未开启搜索增强](../../screenshots/search-enhancement-disabled.png)

开启本地搜索增强后：
![开启搜索增强](../../screenshots/search-enhancement-enabled.png)

可以看到，开启搜索增强后，AI 会自动找到笔记库中的相关信息（显示为「Relevant information found by local search」），并基于这些本地知识给出更准确、更个性化的回答。

### 3. 相关文件智能建议

**添加上下文时：**
```
您添加: "项目规划.md"
AI 建议: 💡 相关文件（5秒内可添加）：
    - 敏捷开发流程.md
    - 任务管理系统.md
    - 团队协作指南.md
```

### 4. 速读功能增强

启用搜索增强后，速读报告会自动：
- 引用笔记库中的相关内容
- 在"延伸阅读"中推荐相关笔记
- 提供更丰富的上下文信息

---

## ⚙️ 高级配置

### 搜索结果设置

```yaml
搜索结果数: 5（默认）
  - 每次搜索返回的相似块数量
  - 范围: 1-20

上下文摘录长度: 500（默认）
  - 发送给 AI 的每个摘录的最大字符数
  - 设置为 0 则发送完整块内容
  - 较短的摘录可减少 Token 使用
```

### 文件过滤器

**包含模式：**
```yaml
- "**/*.md"       # 所有 Markdown 文件
- "**/*.txt"      # 文本文件
```

**排除模式：**
```yaml
- "**/templates/**"  # 模板文件夹
- "**/.obsidian/**"  # Obsidian 配置
- "**/archive/**"    # 归档内容
```

### 性能优化

**索引性能：**
```yaml
批量处理大小: 10    # 一次处理的文件数
并行处理数: 4       # 并发操作数
```

**搜索性能：**
```yaml
缓存超时: 3600秒    # 搜索结果缓存时间
最小相似度: 0.7     # 相似度阈值（0-1）
```

---

## 📊 索引管理

### 更新索引（差异同步）

**增量更新：**
```
命令: "更新索引（差异）"
作用: 仅更新自上次同步以来修改的文件
速度: 快速（秒级）
适用: 日常使用
```

### 重建索引（完整重建）

**完全重建：**
```
命令: "重建索引（完整）"
作用: 清空并从头重建所有索引
速度: 较慢（分钟级）
适用: 更改 Embedding 模型或分块策略后
```

### 索引状态查看

**统计信息：**
```
总文件数: 280
向量块数: 1,247
索引大小: 12.4 MB
上次同步: 5 分钟前
状态: 🟢 最新
```

**暂停/恢复索引：**
- 索引过程中可随时暂停
- 暂停后可继续未完成的索引
- 不会丢失已索引的内容

---

## 💰 成本考虑

### Embedding 成本

**OpenAI 定价：**
```
text-embedding-3-small: $0.02 / 100万 Token

示例笔记库（1000 个笔记，50万字）：
  ≈ 67万 Token
  首次索引成本: ~$0.013（一次性）
  
日常更新（每天 10 个笔记）：
  ≈ 6.7K Token/天
  月度成本: ~$0.05/月
```

**本地 Transformer 模型：**
```
完全免费
无 API 成本
仅占用本地存储和计算资源
```

**Ollama：**
```
免费（仅硬件成本）
无 API 调用费用
```

---

## 🐛 故障排除

### 索引构建问题

**问题：索引构建失败**
```
错误: 无法嵌入文件: large-file.md
```

**解决方案：**
1. ✅ 检查文件大小，排除过大文件
2. ✅ 减小分块大小
3. ✅ 检查 Embedding 模型是否正常
4. ✅ 查看控制台错误日志

**问题：索引速度慢**
```
进度: 10 个文件/分钟（预期: 50/分钟）
```

**解决方案：**
1. ✅ 使用本地 Transformer 模型（更快）
2. ✅ 增加并行处理数
3. ✅ 排除不必要的文件
4. ✅ 检查网络连接（如使用远程 API）

### 搜索问题

**问题：未找到相似笔记**
```
当前笔记: 机器学习入门
相似笔记: 未找到
```

**解决方案：**
1. ✅ 确认已启用"显示相似笔记"
2. ✅ 降低相似度阈值
3. ✅ 检查笔记是否已索引
4. ✅ 尝试重建索引

**问题：搜索结果不相关**
```
查询: "项目管理"
结果: 显示了不相关的笔记
```

**解决方案：**
1. ✅ 提高相似度阈值
2. ✅ 使用更好的 Embedding 模型（OpenAI）
3. ✅ 检查分块策略是否合适
4. ✅ 确保笔记内容质量

---

## 📚 最佳实践

### 🎯 索引策略

1. **从核心内容开始**
   - 先索引重要的知识笔记
   - 排除临时文件和草稿
   - 逐步扩展索引范围

2. **定期维护**
   - 每周进行一次差异同步
   - 每月或更改模型后完全重建
   - 清理不需要的笔记

3. **质量优于数量**
   - 保持笔记结构清晰
   - 使用有意义的标题和段落
   - 排除低价值内容

### 🚀 使用技巧

1. **结合手动上下文**
   - 搜索增强 + 手动添加文件
   - 交叉验证相关性
   - 利用相关文件建议

2. **优化分块策略**
   - 结构化笔记用语义分块
   - 长文档考虑字符分块
   - 根据实际效果调整

3. **监控性能**
   - 定期查看索引统计
   - 注意搜索质量
   - 根据需要调整参数

---

## 🔒 隐私与安全

### 数据存储

- **本地优先**：所有向量索引存储在本地（`.obsidian/plugins/obsidian-llmsider/vector-data/`）
- **可选远程**：仅在使用 OpenAI Embedding 时，原文发送到 API 进行向量化
- **完全离线**：使用本地 Transformer 模型时，所有处理都在本地完成

### 推荐配置

**高隐私需求：**
```yaml
Embedding 模型: 本地 Transformer 模型
分块策略: 语义分块
数据传输: 零
```

**平衡性能与隐私：**
```yaml
Embedding 模型: OpenAI（仅向量化时发送）
其他处理: 全部本地
实时对话: AI 不自动访问索引
```

---

## 📖 相关指南

- [连接与模型](connections-and-models.md) - Embedding 模型配置
- [聊天界面](chat-interface.md) - 在对话中使用搜索增强
- [速读功能](speed-reading.md) - 速读如何利用搜索增强
- [设置指南](settings-guide.md) - 完整配置参考

---

**需要帮助？** [GitHub Issues](https://github.com/gnuhpc/obsidian-llmsider/issues)
