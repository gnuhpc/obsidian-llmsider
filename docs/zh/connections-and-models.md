# 🔗 连接与模型指南

## 概述

LLMSider 使用灵活的 **连接 + 模型** 架构,允许您:
- 同时连接到多个 AI 提供商
- 为每个提供商配置多个模型
- 即时切换模型
- 独立管理 API 密钥和设置

---

## 🎯 关键概念

### 连接
**连接** 表示与 AI 提供商 API 的链接。它存储:
- 提供商类型(OpenAI、Anthropic 等)
- API 凭据(密钥、端点)
- 基础 URL 和配置
- 连接特定的设置

### 模型
**模型** 是连接中的特定 AI 模型配置:
- 模型名称(例如 gpt-4、claude-3-5-sonnet)
- 参数(温度、最大令牌数)
- 模型特定设置
- 嵌入模型 vs 聊天模型指定

---

## 🚀 设置连接

### 步骤 1: 打开设置
1. 点击 Obsidian 的齿轮图标
2. 导航到 **LLMSider** 部分
3. 在顶部找到 **连接和模型** 部分

### 步骤 2: 添加连接

#### 选项 B: 支持的提供商

1. **OpenAI**
   ```
   API 密钥: sk-...
   基础 URL: https://api.openai.com/v1 (默认)
   ```

2. **Anthropic (Claude)**
   ```
   API 密钥: sk-ant-...
   基础 URL: https://api.anthropic.com (默认)
   ```

3. **GitHub Copilot**
   ```
   GitHub 令牌: ghp_...
   (Copilot 令牌自动刷新)
   ```

4. **Azure OpenAI**
   ```
   API 密钥: 您的 Azure API 密钥
   端点: https://<resource>.openai.azure.com
   部署名称: 您的部署名称
   API 版本: 2024-02-15-preview
   ```

5. **Gemini**
   ```
   API 密钥: AIza...
   基础 URL: https://generativelanguage.googleapis.com/v1
   ```

6. **Groq**
   ```
   API 密钥: gsk_...
   基础 URL: https://api.groq.com/openai/v1
   ```

7. **Ollama (本地)**
   ```
   基础 URL: http://localhost:11434
   (无需 API 密钥)
   ```

8. **Qwen (通义千问)**
   ```
   API 密钥: sk-...
   基础 URL: https://dashscope.aliyuncs.com/compatible-mode/v1
   ```

9. **Hugging Face (嵌入)**
   ```
   API 密钥: hf_...
   模型: sentence-transformers/all-MiniLM-L6-v2
   ```

10. **OpenAI 兼容**
    ```
    基础 URL: 您的自定义端点
    API 密钥: 您的 API 密钥(如果需要)
    ```

### 步骤 3: 配置连接设置

填写连接表单:

| 字段 | 描述 | 必需 |
|-------|-------------|----------|
| **名称** | 此连接的友好名称 | ✅ |
| **类型** | 提供商类型(从卡片自动填充) | ✅ |
| **API 密钥** | 您的提供商 API 密钥 | ✅* |
| **基础 URL** | API 端点(有智能默认值) | ⚠️ |
| **已启用** | 切换以启用/禁用连接 | ✅ |

*Ollama 或自定义本地设置不需要

---

## 🎨 管理模型

### 添加模型

1. **点击"添加模型"** - 在任何连接下
2. **填写模型详细信息**:

   | 字段 | 描述 | 示例 |
   |-------|-------------|---------|
   | **名称** | 显示名称 | GPT-4 Turbo |
   | **模型名称** | API 模型标识符 | gpt-4-1106-preview |
   | **类型** | 聊天或嵌入 | 聊天 |
   | **温度** | 创造力(0-2) | 0.7 |
   | **最大令牌数** | 响应长度限制 | 4096 |

3. **设为默认**(可选): 标记为此连接的首选模型

### 模型参数

#### 温度
控制响应的随机性:
- `0.0` - 确定性、一致
- `0.5-0.8` - 平衡(推荐)
- `1.0+` - 创造性、多样化

#### 最大令牌数
最大响应长度:
- **聊天模型**: 通常 1000-4096 令牌
- **嵌入模型**: 改为设置嵌入维度

---

## 🎯 使用多个模型

### 快速切换

**在聊天界面中:**
1. 查看顶部的提供商标签
2. 点击任何标签切换活动模型
3. 当前对话无缝继续

**通过设置:**
1. 启用/禁用连接和模型
2. 更改立即应用

### 模型选择策略

| 用例 | 推荐模型 | 原因 |
|----------|-------------------|-----|
| **快速响应** | GPT-3.5 Turbo | 快速、经济 |
| **复杂推理** | GPT-4, Claude 3 Opus | 卓越的逻辑 |
| **代码生成** | GPT-4, Claude 3.5 Sonnet | 代码理解 |
| **长上下文** | Claude 3 (200K 令牌) | 大文档分析 |
| **本地隐私** | Ollama (Llama 3) | 无 API 调用 |
| **嵌入** | text-embedding-3-small | 质量 + 速度 |

---

## 🔧 高级配置

### 连接级功能

#### 自动重连
自动重新连接断开的连接

#### 速率限制
内置速率限制以遵守 API 配额

#### 令牌刷新 (GitHub Copilot)
到期前自动令牌更新

---

## 💡 提示和最佳实践

### 🎯 连接管理

1. **使用描述性名称**
   ```
   ✅ "OpenAI GPT-4 个人"
   ❌ "连接 1"
   ```

2. **按用途组织**
   - `工作 - Azure OpenAI`
   - `个人 - OpenAI`
   - `本地 - Ollama`

3. **设置合理限制**
   - 不要超过提供商的速率限制
   - 为探索性聊天使用较低的最大令牌数
   - 为最终草稿保留高令牌数

### 🔒 安全最佳实践

1. **API 密钥**
   - 永远不要共享您的 API 密钥
   - 定期轮换密钥
   - 如果可能,使用环境变量存储密钥

2. **访问控制**
   - 不需要时禁用连接
   - 为不同项目使用单独的密钥
   - 定期监控 API 使用情况

3. **本地替代方案**
   - 考虑使用 Ollama 处理敏感数据
   - 使用 OpenAI 兼容的本地服务器
   - 自托管嵌入模型

---

## 🐛 故障排除

### 连接问题

**问题**: 连接失败
- ✅ 验证 API 密钥正确
- ✅ 检查基础 URL 格式
- ✅ 确保网络连接
- ✅ 检查提供商状态页面

**问题**: 响应慢
- ✅ 检查网络延迟
- ✅ 尝试不同的模型
- ✅ 减少最大令牌数
- ✅ 禁用不必要的工具

---

## 📚 延伸阅读

- [聊天界面指南](chat-interface.md)
- [自动补全设置](autocomplete.md)
- [MCP 集成](mcp-integration.md)
- [故障排除指南](troubleshooting.md)

---

**需要帮助?** [打开问题](https://github.com/llmsider/obsidian-llmsider/issues) 或加入我们的 [Discord](https://discord.gg/llmsider)
